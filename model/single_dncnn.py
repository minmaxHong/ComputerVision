# -*- coding: utf-8 -*-
"""single DnCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DXGoY4vlOudvTKWAky05y9DQ8OKWN_55
"""

class DnCNN(nn.Module):
  def __init__(self, num_layers = 17, num_features = 64):
    super(DnCNN, self).__init__()

    # Conv + ReLU
    layers = [nn.Sequential(nn.Conv2d(3, num_features, kernel_size = 3, padding = 1, stride = 1),
                            nn.ReLU(inplace = True))]

    # Conv + BN + ReLU
    for _ in range(num_layers - 2):
      layers.append(nn.Sequential(nn.Conv2d(num_features, num_features, kernel_size = 3, padding = 1, stride = 1),
                                  nn.BatchNorm2d(num_features),
                                  nn.ReLU(inplace = True)))

    # Conv
    layers.append(nn.Conv2d(num_features, 3, kernel_size = 3, padding = 1, stride = 1))

    self.layers = nn.Sequential(*layers)
    self._init_weight()

  def _init_weight(self):
    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight)
      elif isinstance(m, nn.BatchNorm2d):
        nn.init.ones_(m.weight)
        nn.init.zeros_(m.bias)


  def forward(self, inputs):
    y = inputs
    residual = self.layers(y)
    return y - residual